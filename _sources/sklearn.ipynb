{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82970c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ðŸ”¬ Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74d8dd",
   "metadata": {},
   "source": [
    "## What is `Bunch` in `sklearn`?\n",
    "\n",
    "A `Bunch` in `sklearn` is a dictionary-like object used to hold datasets in a structured format. It is returned by functions like `datasets.load_iris()` and other dataset loaders in `sklearn`. The `Bunch` object behaves like a dictionary but provides attribute-style access to its keys, making it user-friendly and intuitive.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Characteristics of `Bunch`:\n",
    "1. **Dictionary-Like Behavior**:  \n",
    "   You can access its elements using dictionary syntax (`bunch['key']`).\n",
    "\n",
    "2. **Attribute-Style Access**:  \n",
    "   Keys in the `Bunch` can also be accessed as attributes (`bunch.key`).\n",
    "\n",
    "3. **Components**:  \n",
    "   A `Bunch` typically contains:\n",
    "   - **`data`**: The main feature matrix as a NumPy array.\n",
    "   - **`target`**: The target labels as a NumPy array.\n",
    "   - **`feature_names`**: Names of the features (columns).\n",
    "   - **`target_names`**: Names of the classes or target labels.\n",
    "   - **`DESCR`**: A description of the dataset.\n",
    "   - **`filename`**: Path to the dataset file (if applicable).\n",
    "\n",
    "4. **Convenience for Machine Learning**:  \n",
    "   The structured nature of a `Bunch` makes it easy to manipulate datasets and integrate them into workflows for machine learning and data analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Using the Iris Dataset as an Example\n",
    "\n",
    "Letâ€™s explore the `Bunch` object returned by `datasets.load_iris()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ed90a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Inspect the type of the object\n",
    "print(type(iris))  # <class 'sklearn.utils.Bunch'>\n",
    "\n",
    "# Accessing keys in the Bunch object\n",
    "print(\n",
    "    iris.keys()\n",
    ")  # dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
    "\n",
    "# Access feature data and target labels\n",
    "print(iris.data[:5])  # First 5 rows of feature matrix (sepal and petal measurements)\n",
    "print(iris.target[:5])  # First 5 target labels (encoded as integers 0, 1, 2)\n",
    "\n",
    "# Access metadata\n",
    "print(\n",
    "    iris.feature_names\n",
    ")  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "print(iris.target_names)  # ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Access description of the dataset\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac187e8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Usage with the Iris Dataset\n",
    "\n",
    "#### **1. Converting `Bunch` to a Pandas DataFrame**\n",
    "You can convert the `Bunch` into a more familiar tabular format for easier analysis:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Convert feature data and target labels to a DataFrame\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add a column for target class labels\n",
    "df['species'] = iris.target\n",
    "\n",
    "# Map numerical target labels to their corresponding class names\n",
    "df['species_name'] = df['species'].map({i: name for i, name in enumerate(iris.target_names)})\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Visualizing the Data**\n",
    "Using the converted DataFrame, we can visualize the features and relationships in the Iris dataset:\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pairplot to visualize feature relationships across species\n",
    "sns.pairplot(df, hue='species_name', diag_kind='kde')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Building a Simple Classifier**\n",
    "We can directly use the `data` and `target` from the `Bunch` to build a machine learning model:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Using `Bunch` in `sklearn`:\n",
    "\n",
    "1. **Organized Access**:  \n",
    "   The `Bunch` structure keeps all relevant components of a dataset (features, labels, metadata) neatly organized in one object.\n",
    "\n",
    "2. **Ease of Use**:  \n",
    "   Attribute-style access reduces boilerplate code, making it easier to explore and use datasets.\n",
    "\n",
    "3. **Compatibility**:  \n",
    "   `Bunch` integrates seamlessly with NumPy, Pandas, and other Python libraries, enabling quick preprocessing and analysis.\n",
    "\n",
    "4. **Standardization**:  \n",
    "   Since all `sklearn` datasets use the `Bunch` format, you can follow consistent workflows regardless of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "The `Bunch` object in `sklearn` provides a structured, dictionary-like interface for handling datasets. Its ability to store both data and metadata makes it a powerful tool for machine learning workflows. By converting it into other formats (like Pandas DataFrame) or directly using its attributes, you can easily integrate the dataset into preprocessing pipelines and model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
